{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from lxml.etree import XMLParser, parse\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the XML file\n",
    "p = XMLParser(huge_tree=True)\n",
    "tree = parse('../data/Posts.xml', parser=p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract elements from the XML tree\n",
    "root = tree.getroot()\n",
    "data = []\n",
    "\n",
    "for post in root.findall('row'):\n",
    "    data.append(post.attrib)\n",
    "\n",
    "# Conver to a pandas DataFrame\n",
    "posts = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column ```Body``` is a raw HTML code. Before applying any model, we need to clean it from redundant tags.\n",
    "\n",
    "For this, we will use a library ```BeautifulSoup```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def html_to_str(row_html: str) -> str:\n",
    "    soup = BeautifulSoup(row_html, 'html.parser')\n",
    "    return soup.get_text(separator=' ')\n",
    "\n",
    "posts[\"Body\"] = posts[\"Body\"].apply(html_to_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = ['Body', 'Tags']\n",
    "posts = posts[columns_to_keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict ```Tags``` based on the ```Body```'s embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_subset = posts.copy()\n",
    "# posts_subset = posts_subset[0:10_000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedder models\n",
    "models = {\n",
    "    'Albert': 'paraphrase-albert-small-v2',\n",
    "    'Roberta': 'all-distilroberta-v1',\n",
    "    'DistilBert': 'multi-qa-distilbert-cos-v1',\n",
    "    'MiniLM1': 'all-MiniLM-L6-v2',\n",
    "    'MiniLM2': 'all-MiniLM-L12-v2',\n",
    "    'MiniLM3': 'paraphrase-MiniLM-L3-v2'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings for Albert are already dumped.\n",
      "Embeddings for Roberta are already dumped.\n",
      "Embeddings for DistilBert are already dumped.\n",
      "Embeddings for MiniLM1 are already dumped.\n",
      "Embeddings for MiniLM2 are already dumped.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding posts: 100%|██████████| 112485/112485 [14:39<00:00, 127.85it/s]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import collections.abc\n",
    "from pathlib import Path\n",
    "\n",
    "def dump_embeddings(models_name: collections.abc.Iterable) -> None:\n",
    "    for model_name in models_name:\n",
    "\n",
    "        # Check if embeddings are already dumped (using this model)\n",
    "        embeddings_file = Path(f\"embeddings/{model_name}_body.obj\")\n",
    "        if embeddings_file.is_file():\n",
    "            # file exists\n",
    "            print(f\"Embeddings for {model_name} are already dumped.\")\n",
    "            continue\n",
    "\n",
    "        # Download a model from Hugging Face using its name\n",
    "        embedder = SentenceTransformer(models[model_name])\n",
    "\n",
    "        bodies = posts_subset['Body'].tolist()\n",
    "\n",
    "        X = []\n",
    "        for body in tqdm(bodies, desc=\"Encoding posts\"):\n",
    "            # Encode each 'body' and append it to X\n",
    "            encoded_body = embedder.encode(body)\n",
    "            X.append(encoded_body)\n",
    "\n",
    "        X = np.array(X)\n",
    "        filehandler = open(f\"embeddings/{model_name}_body.obj\",\"wb\")\n",
    "        pickle.dump(X, filehandler)\n",
    "        filehandler.close()\n",
    "    \n",
    "\n",
    "dump_embeddings(models.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a target vector\n",
    "\n",
    "Our target is a list of lists, where each nested list includes tags for a corresponding question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load embeddings (make sure you dumped it before by running the previous cell)\n",
    "#============================================================================#\n",
    "# This is just an example (I chose a specific model).\n",
    "# The entire code below should be wrapped in a single function.\n",
    "#============================================================================#\n",
    "file = open(\"embeddings/MiniLM3_body.obj\",'rb')\n",
    "X = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = posts_subset[\"Tags\"] # this a list of str, where each str is in the specified format (delimiter = '|')\n",
    "y = [str(str_of_tags).split('|')[1:-1] for str_of_tags in y] # this a list of lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique_tags: 1010\n"
     ]
    }
   ],
   "source": [
    "unique_tags = list(set([\n",
    "    x\n",
    "    for xs in y\n",
    "    for x in xs\n",
    "]))\n",
    "\n",
    "print(f\"unique_tags: {len(unique_tags)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert tags to multi-label format (One-Hot Encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit MultiLabelBinarizer on the full dataset (y) !!\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_full_binary = mlb.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform both train and test sets with the same mlb\n",
    "\n",
    "y_train_binary = mlb.transform(y_train)\n",
    "y_test_binary = mlb.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionTagDataset(Dataset):\n",
    "    def __init__(self, embeddings, tags):\n",
    "        self.embeddings = torch.tensor(embeddings, dtype=torch.float32)\n",
    "        self.tags = torch.tensor(tags, dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.embeddings)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.embeddings[idx], self.tags[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = QuestionTagDataset(X_train, y_train_binary)\n",
    "test_dataset = QuestionTagDataset(X_test, y_test_binary)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Neural Network (4 FC layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TagPredictorNN(nn.Module):\n",
    "    def __init__(self, input_size, num_tags):\n",
    "        super(TagPredictorNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 256)\n",
    "        self.fc3 = nn.Linear(256, 256)\n",
    "        self.fc4 = nn.Linear(256, num_tags)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = torch.sigmoid(self.fc4(x)) # sigmoid is important here! (multi-label classification)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input and output dimensions\n",
    "input_size = X_train.shape[1]  # i.e. 384 (embedding size)\n",
    "num_tags = len(unique_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TagPredictorNN(input_size, num_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, num_epochs=10):\n",
    "    loss_fn = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, (inputs, labels) in enumerate(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/10:.4f}')\n",
    "        running_loss = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "This is a toy example. Increase the number of epochs by a factor of 10, and the dataset should be larger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 1.7473\n",
      "Epoch [2/50], Loss: 0.6834\n",
      "Epoch [3/50], Loss: 0.6183\n",
      "Epoch [4/50], Loss: 0.5800\n",
      "Epoch [5/50], Loss: 0.5535\n",
      "Epoch [6/50], Loss: 0.5316\n",
      "Epoch [7/50], Loss: 0.5132\n",
      "Epoch [8/50], Loss: 0.4965\n",
      "Epoch [9/50], Loss: 0.4796\n",
      "Epoch [10/50], Loss: 0.4648\n",
      "Epoch [11/50], Loss: 0.4507\n",
      "Epoch [12/50], Loss: 0.4370\n",
      "Epoch [13/50], Loss: 0.4242\n",
      "Epoch [14/50], Loss: 0.4122\n",
      "Epoch [15/50], Loss: 0.3999\n",
      "Epoch [16/50], Loss: 0.3882\n",
      "Epoch [17/50], Loss: 0.3777\n",
      "Epoch [18/50], Loss: 0.3665\n",
      "Epoch [19/50], Loss: 0.3567\n",
      "Epoch [20/50], Loss: 0.3471\n",
      "Epoch [21/50], Loss: 0.3385\n",
      "Epoch [22/50], Loss: 0.3293\n",
      "Epoch [23/50], Loss: 0.3206\n",
      "Epoch [24/50], Loss: 0.3136\n",
      "Epoch [25/50], Loss: 0.3060\n",
      "Epoch [26/50], Loss: 0.2983\n",
      "Epoch [27/50], Loss: 0.2919\n",
      "Epoch [28/50], Loss: 0.2846\n",
      "Epoch [29/50], Loss: 0.2787\n",
      "Epoch [30/50], Loss: 0.2729\n",
      "Epoch [31/50], Loss: 0.2674\n",
      "Epoch [32/50], Loss: 0.2622\n",
      "Epoch [33/50], Loss: 0.2572\n",
      "Epoch [34/50], Loss: 0.2525\n",
      "Epoch [35/50], Loss: 0.2467\n",
      "Epoch [36/50], Loss: 0.2427\n",
      "Epoch [37/50], Loss: 0.2385\n",
      "Epoch [38/50], Loss: 0.2353\n",
      "Epoch [39/50], Loss: 0.2303\n",
      "Epoch [40/50], Loss: 0.2270\n",
      "Epoch [41/50], Loss: 0.2237\n",
      "Epoch [42/50], Loss: 0.2202\n",
      "Epoch [43/50], Loss: 0.2168\n",
      "Epoch [44/50], Loss: 0.2134\n",
      "Epoch [45/50], Loss: 0.2103\n",
      "Epoch [46/50], Loss: 0.2080\n",
      "Epoch [47/50], Loss: 0.2049\n",
      "Epoch [48/50], Loss: 0.2032\n",
      "Epoch [49/50], Loss: 0.2013\n",
      "Epoch [50/50], Loss: 0.1974\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import jaccard_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, threshold=0.5):\n",
    "    model.eval()\n",
    "\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_test_tensor) # Forward pass\n",
    "\n",
    "    # Threshold is applied to convert probabilities to binary predictions\n",
    "    y_pred_binary = (y_pred > threshold).float().numpy()\n",
    "    y_true_binary = y_test_tensor.numpy()\n",
    "\n",
    "    # Compute different metrics...\n",
    "    jaccard = jaccard_score(y_true_binary, y_pred_binary, average='samples')\n",
    "    precision = precision_score(y_true_binary, y_pred_binary, average='samples')\n",
    "    recall = recall_score(y_true_binary, y_pred_binary, average='samples')\n",
    "    f1 = f1_score(y_true_binary, y_pred_binary, average='samples')\n",
    "\n",
    "    print(f'Jaccard Index: {jaccard:.4f}')\n",
    "    print(f'Precision: {precision:.4f}')\n",
    "    print(f'Recall: {recall:.4f}')\n",
    "    print(f'F1 Score: {f1:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===TEST===\n",
      "Jaccard Index: 0.0749\n",
      "Precision: 0.1315\n",
      "Recall: 0.1016\n",
      "F1 Score: 0.1043\n",
      "==========\n",
      "===TRAIN===\n",
      "Jaccard Index: 0.2187\n",
      "Precision: 0.2880\n",
      "Recall: 0.2386\n",
      "F1 Score: 0.2518\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "print(\"===TEST===\")\n",
    "evaluate_model(model, X_test, y_test_binary)\n",
    "print(\"==========\")\n",
    "print(\"===TRAIN===\")\n",
    "evaluate_model(model, X_train, y_train_binary)\n",
    "print(\"==========\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
