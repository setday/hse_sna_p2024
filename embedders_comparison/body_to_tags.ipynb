{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from lxml.etree import XMLParser, parse\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the XML file\n",
    "p = XMLParser(huge_tree=True)\n",
    "tree = parse('../data/Posts.xml', parser=p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract elements from the XML tree\n",
    "root = tree.getroot()\n",
    "data = []\n",
    "\n",
    "for post in root.findall('row'):\n",
    "    data.append(post.attrib)\n",
    "\n",
    "# Conver to a pandas DataFrame\n",
    "posts = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column ```Body``` is a raw HTML code. Before applying any model, we need to clean it from redundant tags.\n",
    "\n",
    "For this, we will use a library ```BeautifulSoup```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def html_to_str(row_html: str) -> str:\n",
    "    soup = BeautifulSoup(row_html, 'html.parser')\n",
    "    return soup.get_text(separator=' ')\n",
    "\n",
    "posts[\"Body\"] = posts[\"Body\"].apply(html_to_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = ['Body', 'Tags']\n",
    "posts = posts[columns_to_keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict ```Tags``` based on the ```Body```'s embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_subset = posts.copy()\n",
    "posts_subset = posts_subset[0:10_000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedder models\n",
    "models = {\n",
    "    'Albert': 'paraphrase-albert-small-v2',\n",
    "    'Roberta': 'all-distilroberta-v1',\n",
    "    'DistilBert': 'multi-qa-distilbert-cos-v1',\n",
    "    'MiniLM1': 'all-MiniLM-L6-v2',\n",
    "    'MiniLM2': 'all-MiniLM-L12-v2',\n",
    "    'MiniLM3': 'paraphrase-MiniLM-L3-v2'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings for Albert are already dumped.\n",
      "Embeddings for Roberta are already dumped.\n",
      "Embeddings for DistilBert are already dumped.\n",
      "Embeddings for MiniLM1 are already dumped.\n",
      "Embeddings for MiniLM2 are already dumped.\n",
      "Embeddings for MiniLM3 are already dumped.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import collections.abc\n",
    "from pathlib import Path\n",
    "\n",
    "def dump_embeddings(models_name: collections.abc.Iterable) -> None:\n",
    "    for model_name in models_name:\n",
    "\n",
    "        # Check if embeddings are already dumped (using this model)\n",
    "        embeddings_file = Path(f\"embeddings/{model_name}_body.obj\")\n",
    "        if embeddings_file.is_file():\n",
    "            # file exists\n",
    "            print(f\"Embeddings for {model_name} are already dumped.\")\n",
    "            continue\n",
    "\n",
    "        # Download a model from Hugging Face using its name\n",
    "        embedder = SentenceTransformer(models[model_name])\n",
    "\n",
    "        bodies = posts_subset['Body'].tolist()\n",
    "\n",
    "        X = []\n",
    "        for body in tqdm(bodies, desc=\"Encoding posts\"):\n",
    "            # Encode each 'body' and append it to X\n",
    "            encoded_body = embedder.encode(body)\n",
    "            X.append(encoded_body)\n",
    "\n",
    "        X = np.array(X)\n",
    "        filehandler = open(f\"embeddings/{model_name}_body.obj\",\"wb\")\n",
    "        pickle.dump(X, filehandler)\n",
    "        filehandler.close()\n",
    "    \n",
    "\n",
    "dump_embeddings(models.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a target vector\n",
    "\n",
    "Our target is a list of lists, where each nested list includes tags for a corresponding question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load embeddings (make sure you dumped it before by running the previous cell)\n",
    "#============================================================================#\n",
    "# This is just an example (I chose a specific model).\n",
    "# The entire code below should be wrapped in a single function.\n",
    "#============================================================================#\n",
    "file = open(\"embeddings/MiniLM3_body.obj\",'rb')\n",
    "X = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = posts_subset[\"Tags\"] # this a list of str, where each str is in the specified format (delimiter = '|')\n",
    "y = [str(str_of_tags).split('|')[1:-1] for str_of_tags in y] # this a list of lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique_tags: 589\n"
     ]
    }
   ],
   "source": [
    "unique_tags = list(set([\n",
    "    x\n",
    "    for xs in y\n",
    "    for x in xs\n",
    "]))\n",
    "\n",
    "print(f\"unique_tags: {len(unique_tags)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert tags to multi-label format (One-Hot Encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit MultiLabelBinarizer on the full dataset (y) !!\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_full_binary = mlb.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform both train and test sets with the same mlb\n",
    "\n",
    "y_train_binary = mlb.transform(y_train)\n",
    "y_test_binary = mlb.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionTagDataset(Dataset):\n",
    "    def __init__(self, embeddings, tags):\n",
    "        self.embeddings = torch.tensor(embeddings, dtype=torch.float32)\n",
    "        self.tags = torch.tensor(tags, dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.embeddings)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.embeddings[idx], self.tags[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = QuestionTagDataset(X_train, y_train_binary)\n",
    "test_dataset = QuestionTagDataset(X_test, y_test_binary)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Neural Network (2 FC layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TagPredictorNN(nn.Module):\n",
    "    def __init__(self, input_size, num_tags):\n",
    "        super(TagPredictorNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, num_tags)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x)) # sigmoid is important here! (multi-label classification)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input and output dimensions\n",
    "input_size = X_train.shape[1]  # i.e. 384 (embedding size)\n",
    "num_tags = len(unique_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TagPredictorNN(input_size, num_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, num_epochs=10):\n",
    "    loss_fn = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, (inputs, labels) in enumerate(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/10:.4f}')\n",
    "        running_loss = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "This is a toy example. Increase the number of epochs by a factor of 10, and the dataset should be larger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 6.1156\n",
      "Epoch [2/5], Loss: 2.8180\n",
      "Epoch [3/5], Loss: 2.3318\n",
      "Epoch [4/5], Loss: 1.9807\n",
      "Epoch [5/5], Loss: 1.7010\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, num_epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard Index: 0.0360\n",
      "Precision: 0.0825\n",
      "Recall: 0.0382\n",
      "F1 Score: 0.0494\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import jaccard_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, threshold=0.5):\n",
    "    model.eval()\n",
    "\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_test_tensor) # Forward pass\n",
    "\n",
    "    # Threshold is applied to convert probabilities to binary predictions\n",
    "    y_pred_binary = (y_pred > threshold).float().numpy()\n",
    "    y_true_binary = y_test_tensor.numpy()\n",
    "\n",
    "    # Compute different metrics...\n",
    "    jaccard = jaccard_score(y_true_binary, y_pred_binary, average='samples')\n",
    "    precision = precision_score(y_true_binary, y_pred_binary, average='samples')\n",
    "    recall = recall_score(y_true_binary, y_pred_binary, average='samples')\n",
    "    f1 = f1_score(y_true_binary, y_pred_binary, average='samples')\n",
    "\n",
    "    print(f'Jaccard Index: {jaccard:.4f}')\n",
    "    print(f'Precision: {precision:.4f}')\n",
    "    print(f'Recall: {recall:.4f}')\n",
    "    print(f'F1 Score: {f1:.4f}')\n",
    "\n",
    "evaluate_model(model, X_test, y_test_binary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
